{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gfVlOOQ9dL6v",
    "pycharm": {}
   },
   "source": [
    "# Predictions\n",
    "\n",
    "In this notebook you will learn how to build a link prediction classifier using Neo4j and scikit-learn. \n",
    "\n",
    "Import the libraries that you will need (remember to unset Reset all runtimes before running):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_N2PmebJdL6z",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5_oznGPdL65",
    "pycharm": {}
   },
   "source": [
    "Next, create a connection to your Neo4j Sandbox, just as you did previously when you set up your environment. \n",
    "\n",
    "<div align=\"left\">\n",
    "    <img src=\"https://github.com/neo4j-contrib/training-v2/blob/master/Courses/DataScience/notebooks/images/sandbox-citations.png?raw=1\" alt=\"Citation Sandbox\"/>\n",
    "</div>\n",
    "\n",
    "Update the cell below to use the IP Address, Bolt Port, and Password, as you did previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LQCOrCOLdL68",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "# Change the line of code below to use the IP Address, Bolt Port, and Password of your Sandbox.\n",
    "# graph = Graph(\"bolt://<IP Address>:<Bolt Port>\", auth=(\"neo4j\", \"<Password>\")) \n",
    " \n",
    "#graph = Graph(\"bolt://52.3.242.176:33698\", auth=(\"neo4j\", \"equivalent-listing-parts\"))\n",
    "graph = Graph(\"bolt:localhost:11005\", auth=(\"neo4j\", \"graphdb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7V-liIL6dL7B",
    "pycharm": {}
   },
   "source": [
    "# Building a co-author graph\n",
    "\n",
    "You will build an inferred graph of co-authors based on people collaborating on the same papers. You will store a property on the relationship indicating the year of their first collaboration.\n",
    "\n",
    "Run this code to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pePeF7T6dL7C",
    "pycharm": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "constraints_added: 0\n",
       "constraints_removed: 0\n",
       "contained_updates: False\n",
       "indexes_added: 0\n",
       "indexes_removed: 0\n",
       "labels_added: 0\n",
       "labels_removed: 0\n",
       "nodes_created: 0\n",
       "nodes_deleted: 0\n",
       "properties_set: 0\n",
       "relationships_created: 0\n",
       "relationships_deleted: 0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "CALL apoc.periodic.iterate(\n",
    "  \"MATCH (a1)<-[:AUTHOR]-(paper)-[:AUTHOR]->(a2:Author)\n",
    "   WITH a1, a2, paper\n",
    "   ORDER BY a1, paper.year\n",
    "   RETURN a1, a2, collect(paper)[0].year AS year, count(*) AS collaborations\",\n",
    "  \"MERGE (a1)-[coauthor:CO_AUTHOR {year: year}]-(a2)\n",
    "   SET coauthor.collaborations = collaborations\", \n",
    "  {batchSize: 100})\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "   MATCH (a1)<-[:AUTHOR]-(paper)-[:AUTHOR]->(a2:Author)\n",
    "   WITH a1, a2, paper\n",
    "   ORDER BY a1, paper.year\n",
    "   RETURN a1, a2, collect(paper)[0].year AS year, count(*) AS collaborations\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "layb99MHdL7H",
    "pycharm": {}
   },
   "source": [
    "Now that you have created a co-author graph, you need an approach that will allow you to predict future links (relationships) that will be created between people. \n",
    "\n",
    "You will use the [link prediction algorithms](https://neo4j.com/docs/graph-algorithms/current/algorithms/linkprediction/) that you learned about in the previous section. Once you have computed scores with this algorithms what should you do?\n",
    "\n",
    "There are two main approaches that one can take:\n",
    "\n",
    "## Using the measures directly\n",
    "\n",
    "You can use the scores from the link predictions directly, specifying a __threshold value__ above which we predict that a link will be created between two nodes.\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "You can take a supervised learning approach where you use the scores as features to train a binary classifier. The binary classifier then predicts whether a pair of nodes will have a link.\n",
    "\n",
    "In this notebook you will apply the supervised learning approach.\n",
    "\n",
    "## Train and test datasets \n",
    "\n",
    "Next, you must create the train and test datasets on which you can build, and then evaluate a model.\n",
    "\n",
    "## Positive examples\n",
    "\n",
    "The tricky thing when working with graph data is that you cannot just randomly split the data, as this could lead to data leakage.\n",
    "\n",
    "Data leakage can occur when data outside of your training data is inadvertently used to create your model. This can easily happen when working with graphs because pairs of nodes in the training set may be connected to those in the test set.\n",
    "\n",
    "When you compute link prediction measures over that training set the __measures computed contain information from the test set__ that you will later evaluate the model against.\n",
    "\n",
    "Instead, you need to split the graph into training and test sub graphs. If the graph has a concept of time, things are easier as you can split the graph at a point in time. The training set will be from before the time, the test set after.\n",
    "\n",
    "This is still not a perfect solution and you must ensure that the general network structure in the training and test sub graphs is similar.\n",
    "\n",
    "Subsequently, pairs of nodes in our train and test datasets will have relationships between them. They will be the __positive examples__ in your machine learning model.\n",
    "\n",
    "Because the citation graph contains times, you can create train and test graphs by splitting the data on a particular year. Next, you must determine what year that should be. Determine the distribution of the first year that co-authors collaborated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM4uQu9zdL7I",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH p=()-[r:CO_AUTHOR]->() \n",
    "WITH r.year AS year, count(*) AS count\n",
    "ORDER BY year\n",
    "RETURN toString(year) AS year, count\n",
    "\"\"\"\n",
    "by_year = graph.run(query).to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Movie']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']},\n",
       " {'labels(n)': ['Person']}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"\"\"\n",
    "MATCH (n)\n",
    "RETURN labels(n)\n",
    "LIMIT 1000\n",
    "\"\"\"\n",
    "graph.run(query).data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM4uQu9zdL7I",
    "pycharm": {}
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonTF\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ce93ee39b1ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mby_year\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_data_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mby_year\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'bar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'year'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlegend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_label_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonTF\\lib\\site-packages\\pandas\\plotting\\_core.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    757\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m                     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_cols\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 759\u001b[1;33m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    760\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x must be a label or position\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonTF\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2978\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2979\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2980\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2982\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\PythonTF\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'year'"
     ]
    }
   ],
   "source": [
    "ax = by_year.plot(kind='bar', x='year', y='count', legend=None, figsize=(15,8))\n",
    "ax.xaxis.set_label_text(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFy1kopmdL7L",
    "pycharm": {}
   },
   "source": [
    "It looks like 2006 would act as a good year for splitting the data. All co-authorships from 2005 and earlier as our train graph, and everything from 2006 onwards as the test graph.\n",
    "\n",
    "Create explicit `CO_AUTHOR_EARLY` and `CO_AUTHOR_LATE` relationships in the graph based on that year. The following code will create these relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nKoGJEqddL7M",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (a)-[r:CO_AUTHOR]->(b) \n",
    "where r.year < 2006\n",
    "MERGE (a)-[:CO_AUTHOR_EARLY {year: r.year}]-(b);\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query).stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8YUzRZyMdL7Q",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH (a)-[r:CO_AUTHOR]->(b) \n",
    "where r.year >= 2006\n",
    "MERGE (a)-[:CO_AUTHOR_LATE {year: r.year}]-(b);\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query).stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "55XWbEkjdL7T",
    "pycharm": {}
   },
   "source": [
    "Determine how many co-author relationship you have in each of these sub graphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VRl_s1jBdL7U",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH ()-[:CO_AUTHOR_EARLY]->()\n",
    "RETURN count(*) AS count\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query).to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q89n_nJSdL7Y",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "MATCH ()-[:CO_AUTHOR_LATE]->()\n",
    "RETURN count(*) AS count\n",
    "\"\"\"\n",
    "\n",
    "graph.run(query).to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jNu9RT13dL7a",
    "pycharm": {}
   },
   "source": [
    "This graph has a split of 52-48, which is a bit on the high side, but should be ok. Next, we create our __negative examples__.\n",
    "\n",
    "\n",
    "## Negative examples\n",
    "\n",
    "The simplest approach is to use all pair of nodes that don’t have a relationship. __The problem with this approach is that there are significantly more examples of pairs of nodes that don’t have a relationship than there are pairs of nodes that do__.\n",
    "\n",
    "The maximum number of negative examples is equal to:\n",
    "\n",
    "```\n",
    "# negative examples = (# nodes)² - (# relationships) - (# nodes)\n",
    "```\n",
    "\n",
    "i.e. the number of nodes squared, minus the relationships that the graph has, minus self relationships.\n",
    "\n",
    "If you were to use all of these negative examples in your training set, you would have a massive class imbalance — there are many negative examples and relatively few positive ones.\n",
    "\n",
    "A model trained using data that is this imbalanced will achieve very high accuracy by __predicting that any pair of nodes don’t have a relationship__ between them, which is not quite what we want!\n",
    "\n",
    "You need to reduce the number of negative examples. An approach described in several link prediction papers is to use pairs of nodes that are a __specific number of hops away from each other__.\n",
    "\n",
    "This will significantly reduce the number of negative examples, although there will still be a lot more negative examples than positive.\n",
    "\n",
    "To solve this problem, you either need to down sample the negative examples or up sample the positive examples.\n",
    "\n",
    "You will take the down sampling approach. The following function will do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2VhnQWT5dL7b",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def down_sample(df):\n",
    "    copy = df.copy()\n",
    "    zero = Counter(copy.label.values)[0]\n",
    "    un = Counter(copy.label.values)[1]\n",
    "    n = zero - un\n",
    "    copy = copy.drop(copy[copy.label == 0].sample(n=n, random_state=1).index)\n",
    "    return copy.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Efw-Dk-ndL7d",
    "pycharm": {}
   },
   "source": [
    "Now you are ready to build the train and test datasets based on the train and test sub graphs that you created. \n",
    "\n",
    "* The positive examples will be taken directly from the graph. \n",
    "* The negative examples will be found by looking for people who are 2 or 3 hops away from each other, excluding those that have already collaborated. You will then down sample those examples to equal the size of the positive examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MBIJxgxndL7e",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "train_existing_links = graph.run(\"\"\"\n",
    "MATCH (author:Author)-[:CO_AUTHOR_EARLY]->(other:Author)\n",
    "RETURN id(author) AS node1, id(other) AS node2, 1 AS label\n",
    "\"\"\").to_data_frame()\n",
    "\n",
    "train_missing_links = graph.run(\"\"\"\n",
    "MATCH (author:Author)\n",
    "WHERE (author)-[:CO_AUTHOR_EARLY]-()\n",
    "MATCH (author)-[:CO_AUTHOR_EARLY*2..3]-(other)\n",
    "WHERE not((author)-[:CO_AUTHOR_EARLY]-(other))\n",
    "RETURN id(author) AS node1, id(other) AS node2, 0 AS label\n",
    "\"\"\").to_data_frame()\n",
    "train_missing_links = train_missing_links.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Ocig6agdL7h",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df = train_missing_links.append(train_existing_links, ignore_index=True)\n",
    "training_df['label'] = training_df['label'].astype('category')\n",
    "training_df = down_sample(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VnP8quufdL7j",
    "pycharm": {}
   },
   "source": [
    "Now let's have a look what our train DataFrame contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "57uPv25_dL7k",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BJcNR5VYdL7p",
    "pycharm": {}
   },
   "source": [
    "Let's repeat the process for the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XlccbACxdL7q",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_existing_links = graph.run(\"\"\"\n",
    "MATCH (author:Author)-[:CO_AUTHOR_LATE]->(other:Author)\n",
    "RETURN id(author) AS node1, id(other) AS node2, 1 AS label\n",
    "\"\"\").to_data_frame()\n",
    "\n",
    "test_missing_links = graph.run(\"\"\"\n",
    "MATCH (author:Author)\n",
    "WHERE (author)-[:CO_AUTHOR_LATE]-()\n",
    "MATCH (author)-[:CO_AUTHOR_LATE*2..3]-(other)\n",
    "WHERE not((author)-[:CO_AUTHOR_LATE]-(other))\n",
    "RETURN id(author) AS node1, id(other) AS node2, 0 AS label\n",
    "\"\"\").to_data_frame()\n",
    "test_missing_links = test_missing_links.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FWix-40QdL7s",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_df = test_missing_links.append(test_existing_links, ignore_index=True)\n",
    "test_df['label'] = test_df['label'].astype('category')\n",
    "test_df = down_sample(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6ucdxk8NdL7v",
    "pycharm": {}
   },
   "source": [
    "And it's time to sample our test DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kfa9_jEzdL7v",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FLv0PM_hdL7x",
    "pycharm": {}
   },
   "source": [
    "# Choosing a machine learning algorithm\n",
    "\n",
    "Next, you will create a machine learning pipeline based on a random forest classifier. This method is well suited as this data set will be comprised of a mix of strong and weak features. While the weak features will sometimes be helpful, the random forest method will ensure that you don’t create a model that only fits the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rulF5Iz5dL7y",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=30, max_depth=10, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4XoClEY0dL71",
    "pycharm": {}
   },
   "source": [
    "# Generating graphy features\n",
    "\n",
    "Start by creating a simple model that tries to predict whether two authors will have a future collaboration based on features extracted from common authors, preferential attachment, and the total union of neighbors.\n",
    "\n",
    "The following function computes each of these measures for pairs of nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "52AkYrwydL72",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def apply_graphy_features(data, rel_type):\n",
    "    query = \"\"\"\n",
    "    UNWIND $pairs AS pair\n",
    "    MATCH (p1) WHERE id(p1) = pair.node1\n",
    "    MATCH (p2) WHERE id(p2) = pair.node2\n",
    "    RETURN pair.node1 AS node1,\n",
    "           pair.node2 AS node2,\n",
    "           algo.linkprediction.commonNeighbors(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS cn,\n",
    "           algo.linkprediction.preferentialAttachment(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS pa,\n",
    "           algo.linkprediction.totalNeighbors(\n",
    "               p1, p2, {relationshipQuery: $relType}) AS tn\n",
    "    \"\"\"\n",
    "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
    "    features = graph.run(query, {\"pairs\": pairs, \"relType\": rel_type}).to_data_frame()\n",
    "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EWMksQAadL77",
    "pycharm": {}
   },
   "source": [
    "Now apply the function to the training DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R2aKs2jPdL78",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df = apply_graphy_features(training_df, \"CO_AUTHOR_EARLY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PAEGUh4adL7-",
    "pycharm": {}
   },
   "source": [
    "This is what the DataFrame looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eFCsz-vOdL7_",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AssPDF0BdL8B",
    "pycharm": {}
   },
   "source": [
    "Do the same to the test DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cEMTwRyidL8B",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_df = apply_graphy_features(test_df, \"CO_AUTHOR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nd6EHCTPdL8E",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CYHy5h5dL8G",
    "pycharm": {}
   },
   "source": [
    "Next, you will build a model based on these graphy features. You will start by just using one of the features - common neighbors. \n",
    "\n",
    "The following code builds a random forest model, evaluates it against the test dataset, and then indicates which of the features had the most importance in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "czG9o0yOdL8H",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "columns = [\"cn\"]\n",
    "\n",
    "X = training_df[columns]\n",
    "y = training_df[\"label\"]\n",
    "classifier.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTW_3u7_dL8K",
    "pycharm": {}
   },
   "source": [
    "Next, you need to evaluate the model. You will compute its accuracy, precision, and recall. Then, you will return the importance of each feature used in the model. The following functions will help with this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nd-IRmF2dL8K",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, actual):\n",
    "    return pd.DataFrame({\n",
    "        \"Measure\": [\"Accuracy\", \"Precision\", \"Recall\"],\n",
    "        \"Score\": [accuracy_score(actual, predictions), \n",
    "                  precision_score(actual, predictions), \n",
    "                  recall_score(actual, predictions)]\n",
    "    })\n",
    "\n",
    "def feature_importance(columns, classifier):        \n",
    "    display(\"Feature Importance\")\n",
    "    df = pd.DataFrame({\n",
    "        \"Feature\": columns,\n",
    "        \"Importance\": classifier.feature_importances_\n",
    "    })\n",
    "    df = df.sort_values(\"Importance\", ascending=False)    \n",
    "    ax = df.plot(kind='bar', x='Feature', y='Importance', legend=None)\n",
    "    ax.xaxis.set_label_text(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BeEz3lI8dL8M",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "predictions = classifier.predict(test_df[columns])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "display(evaluate_model(predictions, y_test))\n",
    "feature_importance(columns, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7EQcu47XdL8O",
    "pycharm": {}
   },
   "source": [
    "The scores for accuracy and precision are adequate, but the recall is not very good. What happens if you include preferential attachment and total neighbors as well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Trk-cu1dL8P",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "columns = [\"cn\", \"pa\", \"tn\"]\n",
    "\n",
    "X = training_df[columns]\n",
    "y = training_df[\"label\"]\n",
    "classifier.fit(X, y)\n",
    "\n",
    "predictions = classifier.predict(test_df[columns])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "display(evaluate_model(predictions, y_test))\n",
    "feature_importance(columns, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VYLRqlMhdL8R",
    "pycharm": {}
   },
   "source": [
    "Common Neighbors is the dominant feature, but including the two other features has improved the accuracy and recall of the model.\n",
    "\n",
    "Next, you will add some new features that are generated from graph algorithms.\n",
    "\n",
    "# Triangles and The Clustering Coefficient\n",
    "\n",
    "Start by running the [triangle count](https://neo4j.com/docs/graph-algorithms/current/algorithms/triangle-counting-clustering-coefficient/) algorithm over the test and train sub-graphs. This algorithm will return the number of triangles that each node forms, as well as each node's clustering coefficient. The clustering coefficient of a node indicates the likelihood that its neighbors are also connected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VVLMHauzdL8S",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.triangleCount('Author', 'CO_AUTHOR_EARLY', { write:true,\n",
    "writeProperty:'trianglesTrain', clusteringCoefficientProperty:'coefficientTrain'});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AKGE5tA_dL8U",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.triangleCount('Author', 'CO_AUTHOR', { write:true,\n",
    "writeProperty:'trianglesTest', clusteringCoefficientProperty:'coefficientTest'});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QdeiRRdUdL8Z",
    "pycharm": {}
   },
   "source": [
    "The following function will add these features to the train and test DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uiw67Xy9dL8a",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def apply_triangles_features(data, triangles_prop, coefficient_prop):\n",
    "    query = \"\"\"\n",
    "    UNWIND $pairs AS pair\n",
    "    MATCH (p1) WHERE id(p1) = pair.node1\n",
    "    MATCH (p2) WHERE id(p2) = pair.node2\n",
    "    RETURN pair.node1 AS node1,\n",
    "    pair.node2 AS node2,\n",
    "    apoc.coll.min([p1[$trianglesProp], p2[$trianglesProp]]) AS minTriangles,\n",
    "    apoc.coll.max([p1[$trianglesProp], p2[$trianglesProp]]) AS maxTriangles,\n",
    "    apoc.coll.min([p1[$coefficientProp], p2[$coefficientProp]]) AS minCoefficient,\n",
    "    apoc.coll.max([p1[$coefficientProp], p2[$coefficientProp]]) AS maxCoefficient\n",
    "    \"\"\"    \n",
    "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]    \n",
    "    params = {\n",
    "    \"pairs\": pairs,\n",
    "    \"trianglesProp\": triangles_prop,\n",
    "    \"coefficientProp\": coefficient_prop\n",
    "    }\n",
    "    features = graph.run(query, params).to_data_frame()    \n",
    "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFi80UfJdL8c",
    "pycharm": {}
   },
   "source": [
    "Add the new features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sKjonMNwdL8c",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df = apply_triangles_features(training_df, \"trianglesTrain\", \"coefficientTrain\")\n",
    "test_df = apply_triangles_features(test_df, \"trianglesTest\", \"coefficientTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IxuwGwjedL8e",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lnv7mvp_dL8h",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbpDNVDldL8j",
    "pycharm": {}
   },
   "source": [
    "And now let's train and evaluate a model with these features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7d6tT-vAdL8k",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"cn\", \"pa\", \"tn\", # graph features\n",
    "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\" # triangle features  \n",
    "]\n",
    "\n",
    "X = training_df[columns]\n",
    "y = training_df[\"label\"]\n",
    "classifier.fit(X, y)\n",
    "\n",
    "predictions = classifier.predict(test_df[columns])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "display(evaluate_model(predictions, y_test))\n",
    "feature_importance(columns, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEPJbb6SdL8m",
    "pycharm": {}
   },
   "source": [
    "The coefficient features have not added much to our model, but the triangles are useful. Next you will see if Community Detection algorithms can help improve the model.\n",
    "\n",
    "# Community Detection\n",
    "\n",
    "Community Detection algorithms evaluate how a group is clustered or partitioned. Nodes are considered more similar to nodes that fall in their community than to nodes in other communities.\n",
    "\n",
    "You will run two Community Detection algorithms over the train and test sub-graphs - Label Propagation and Louvain. First, Label Propagation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s79HN3uKdL8n",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.labelPropagation(\"Author\", \"CO_AUTHOR_EARLY\", \"BOTH\",\n",
    "{partitionProperty: \"partitionTrain\"});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eyxq8ktRdL8q",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.labelPropagation(\"Author\", \"CO_AUTHOR\", \"BOTH\",\n",
    "{partitionProperty: \"partitionTest\"});\n",
    "\"\"\").to_data_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Huty-y1dL8t",
    "pycharm": {}
   },
   "source": [
    "And now Louvain. The Louvain algorithm returns intermediate communities, which are useful for finding fine grained communities that exist in a graph. You will add a property to each node containing the community revealed on the first iteration of the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZD1YR4badL8u",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.louvain.stream(\"Author\", \"CO_AUTHOR_EARLY\", {includeIntermediateCommunities:true})\n",
    "YIELD nodeId, community, communities\n",
    "WITH algo.getNodeById(nodeId) AS node, communities[0] AS smallestCommunity\n",
    "SET node.louvainTrain = smallestCommunity;\n",
    "\"\"\").stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9FjhnBJdL8w",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "graph.run(\"\"\"\n",
    "CALL algo.louvain.stream(\"Author\", \"CO_AUTHOR\", {includeIntermediateCommunities:true})\n",
    "YIELD nodeId, community, communities\n",
    "WITH algo.getNodeById(nodeId) AS node, communities[0] AS smallestCommunity\n",
    "SET node.louvainTest = smallestCommunity;\n",
    "\"\"\").stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_Zp_SprdL8z",
    "pycharm": {}
   },
   "source": [
    "The following function will add these features to the train and test DataFrames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1JIoaMQdL80",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "def apply_community_features(data, partition_prop, louvain_prop):\n",
    "    query = \"\"\"\n",
    "    UNWIND $pairs AS pair\n",
    "    MATCH (p1) WHERE id(p1) = pair.node1\n",
    "    MATCH (p2) WHERE id(p2) = pair.node2\n",
    "    RETURN pair.node1 AS node1,\n",
    "    pair.node2 AS node2,\n",
    "    algo.linkprediction.sameCommunity(p1, p2, $partitionProp) AS sp,    \n",
    "    algo.linkprediction.sameCommunity(p1, p2, $louvainProp) AS sl\n",
    "    \"\"\"\n",
    "    pairs = [{\"node1\": node1, \"node2\": node2}  for node1,node2 in data[[\"node1\", \"node2\"]].values.tolist()]\n",
    "    params = {\n",
    "    \"pairs\": pairs,\n",
    "    \"partitionProp\": partition_prop,\n",
    "    \"louvainProp\": louvain_prop\n",
    "    }\n",
    "    features = graph.run(query, params).to_data_frame()\n",
    "    return pd.merge(data, features, on = [\"node1\", \"node2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f75iAGnRdL86",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df = apply_community_features(training_df, \"partitionTrain\", \"louvainTrain\")\n",
    "test_df = apply_community_features(test_df, \"partitionTest\", \"louvainTest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "urKrVbSTdL88",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1-ss8GZdL8-",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CQHpytXmdL8_",
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"cn\", \"pa\", \"tn\", # graph features\n",
    "    \"minTriangles\", \"maxTriangles\", \"minCoefficient\", \"maxCoefficient\", # triangle features  \n",
    "    \"sp\", \"sl\" # community features\n",
    "]\n",
    "\n",
    "X = training_df[columns]\n",
    "y = training_df[\"label\"]\n",
    "classifier.fit(X, y)\n",
    "\n",
    "predictions = classifier.predict(test_df[columns])\n",
    "y_test = test_df[\"label\"]\n",
    "\n",
    "display(evaluate_model(predictions, y_test))\n",
    "feature_importance(columns, classifier)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "04_Predictions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
